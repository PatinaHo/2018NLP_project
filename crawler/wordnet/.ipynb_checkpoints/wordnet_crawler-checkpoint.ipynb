{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "         'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y' ,'Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing A\n",
      "processing B\n",
      "processing C\n",
      "processing D\n",
      "processing E\n",
      "processing F\n",
      "processing G\n",
      "processing H\n",
      "processing I\n",
      "processing J\n",
      "processing K\n",
      "processing L\n",
      "processing M\n",
      "processing N\n",
      "processing O\n",
      "processing P\n",
      "processing Q\n",
      "processing R\n",
      "processing S\n",
      "processing T\n",
      "processing U\n",
      "processing V\n",
      "processing W\n",
      "processing X\n",
      "processing Y\n",
      "processing Z\n"
     ]
    }
   ],
   "source": [
    "for alpha in alpha_list:\n",
    "    if alpha != 'A': continue\n",
    "    json_data = json.load(open('../EVP/data/'+alpha+'.json', 'r')) \n",
    "    data = open('data/'+alpha+'.txt', 'w')\n",
    "    print('processing %s' % alpha)\n",
    "    \n",
    "    data.write('word\tpos\tfreq\tWN def\texam\\n')\n",
    "    data.write('========================================================================\\n')\n",
    "    \n",
    "    for w in json_data:\n",
    "        word = w['word']\n",
    "        if word == 'a':print(word)\n",
    "        # EVP data number \n",
    "        # find max\n",
    "        sense_num = -1\n",
    "        for p in w['poses']:\n",
    "            num = len(p['senses'])\n",
    "            if num > sense_num:\n",
    "                sense_num = num\n",
    "        \n",
    "        syns = wordnet.synsets(word)\n",
    "        \n",
    "        for sense_index, s in enumerate(syns, start=1):\n",
    "            pos = s.pos()\n",
    "            sense = '(' + s.definition() + ')'\n",
    "            \n",
    "            exams = s.examples()\n",
    "            new_exam = []\n",
    "            for e in exams:\n",
    "                new_exam.append('\"' + e + '\"')\n",
    "                \n",
    "            freq = 0\n",
    "            for l in s.lemmas():\n",
    "                if l.name() == word:\n",
    "                    freq = l.count()\n",
    "                    break\n",
    "            if sense_num > sense_index+3: break\n",
    "            \n",
    "            word_set = [word, pos, str(freq), 'W'+str(sense_index), sense, '; '.join(new_exam)]\n",
    "                \n",
    "            data.write(' '.join(word_set) + '\\n')\n",
    "    \n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
