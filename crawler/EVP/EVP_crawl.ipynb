{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# driver\n",
    "driver.page_source  \n",
    "driver.current_url  \n",
    "driver.find_element_by_id(\"id\").click()\n",
    "# beautifulsoup\n",
    "a['href']  \n",
    "soup.find_all('b', class_=\"pos\")  \n",
    "a.text 可為該tag中的text，若是tag中間含有其他tag，他們的text也會被包含在內  \n",
    "lear_examp.div.extract() 在lear_examp中將div抽出來並返回  \n",
    "lear_examp.div.decompose() 在lear_examp中將div抽出來摧毀並不返回\n",
    "# json\n",
    "json.dumps([item.reprJSON() for item in word_list], cls=ComplexEncoder)  \n",
    "``` python\n",
    "class ComplexEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj,'reprJSON'):\n",
    "            return obj.reprJSON()\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "```\n",
    "# re\n",
    "利用re模块分割含有多种分割符的字符串：\n",
    "``` python\n",
    "import re\n",
    "a='Beautiful, is; better*than\\nugly'\n",
    "x= re.split(',|; |\\*|\\n',a)\n",
    "print(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"englishprofile\"\n",
    "password = \"vocabulary\"\n",
    "url = \"http://\" + username + \":\" + password + \"@vocabulary.englishprofile.org/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhantomJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.PhantomJS(executable_path=r'/usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs')  # PhantomJs\n",
    "# driver.get(url)  # 輸入範例網址，交給瀏覽器 \n",
    "# pageSource = driver.page_source  # 取得網頁原始碼\n",
    "# print(pageSource)\n",
    "\n",
    "# driver.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=r'/usr/local/share/chrome_driver/chromedriver')\n",
    "#driver = webdriver.Chrome(executable_path=r'C:\\Users\\USER\\Desktop\\chromedriver')\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# from uk to us\n",
    "driver.find_element_by_id(\"lang-us-btn\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "# browse a-z\n",
    "driver.find_element_by_id(\"id_browse\").click()\n",
    "time.sleep(2)\n",
    "a2z_html = driver.page_source  # 取得網頁原始碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "alpha_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "         'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y' ,'Z']\n",
    "prefix_alpha = 'http://vocabulary.englishprofile.org/dictionary//word-list/us/a1_c2/'\n",
    "prefix_sub = 'http://vocabulary.englishprofile.org'\n",
    "\n",
    "# unused\n",
    "pos_dict = {'determiner':'-det.', 'verb':'-v.', 'adjective':'-adj.', 'adverb':'-adv.', \n",
    "            'noun': '-n.', 'exclamation': '-excl.', 'conjunction': '-conj.', \n",
    "            'pronoun': '-pron.', 'preposition': '-prep.', }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a2z: show a to z link  \n",
    "list: means some prefix alpha, which seperate several word into n sublists  \n",
    "sublist: composed by several words  \n",
    "span_list: list every words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json 格式  \n",
    "- class Word  \n",
    "    - str word  \n",
    "    - POS list poses  \n",
    "- class POS   \n",
    "    - str pos  \n",
    "    - Sense list senses  \n",
    "- class Sense  \n",
    "    - str sense  \n",
    "    - str level  \n",
    "    - str dict_examp  \n",
    "    - str lear_examp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word(object):\n",
    "    def __init__(self, word, poses):\n",
    "        self.word = word\n",
    "        self.poses = poses\n",
    "    def reprJSON(self):\n",
    "        return dict(word=self.word, poses=self.poses)\n",
    "    \n",
    "class POS(object):\n",
    "    def __init__(self, pos, senses):\n",
    "        self.pos = pos\n",
    "        self.senses = senses\n",
    "    def reprJSON(self):\n",
    "        return dict(pos=self.pos, senses=self.senses) \n",
    "\n",
    "class Sense(object):\n",
    "    def __init__(self, sense, level, dict_examp, lear_examp):\n",
    "        self.sense = sense\n",
    "        self.level = level\n",
    "        self.dict_examp = dict_examp\n",
    "        self.lear_examp = lear_examp\n",
    "    def reprJSON(self):\n",
    "        return dict(sense=self.sense, level=self.level, dict_examp=self.dict_examp, lear_examp=self.lear_examp) \n",
    "\n",
    "class ComplexEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj,'reprJSON'):\n",
    "            return obj.reprJSON()\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## html 內容\n",
    "- a class = 'lookup' : 單字\n",
    "- b class = 'pos': 詞性\n",
    "    - div class = 'sense'\n",
    "        - span class = 'def': 由多個 a 組成\n",
    "        - span class = 'freq-A1': 分級\n",
    "        - blockqoute class = 'examp': dict example 有的沒有\n",
    "        - blockqoute class = 'clc': learner example 有的沒有\n",
    "        \n",
    "popcorn no pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove [ ] \\n \\t\n",
    "def remove_char(string):\n",
    "    string = re.sub('\\[|\\]|\\n|\\t', '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing A\n",
      "processing B\n",
      "processing C\n",
      "processing D\n",
      "processing E\n",
      "processing F\n",
      "processing G\n",
      "processing H\n",
      "processing I\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-beecb0ada22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# lookup word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mlookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mword_pos_list\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for index, alpha in enumerate(alpha_list):\n",
    "    print('processing %s' % alpha)\n",
    "    if alpha not in 'IPS': continue #miss i\n",
    "    alpha_url = prefix_alpha + alpha\n",
    "    driver.get(alpha_url)\n",
    "    time.sleep(2)\n",
    "    alpha_html = driver.page_source\n",
    "    \n",
    "    soup_a2z = BeautifulSoup(alpha_html, 'html.parser')\n",
    "    alpha_list = soup_a2z.find_all('a')\n",
    "    \n",
    "    # remove some unneeded url\n",
    "    temp_list = []\n",
    "    for sublist in alpha_list:\n",
    "        if (re.match('/dictionary/word-list/us/a1_c2/' + alpha + '/', sublist['href'])):\n",
    "            temp_list.append(sublist)\n",
    "    alpha_list = temp_list\n",
    "    \n",
    "    word_vocalbulary_list = []\n",
    "    \n",
    "    for sublist in alpha_list:\n",
    "        # crawl every sublist\n",
    "        sublist_url = prefix_sub + sublist['href']\n",
    "        driver.get(sublist_url)\n",
    "        time.sleep(2)\n",
    "        sublist_html = driver.page_source\n",
    "\n",
    "        soup_sublist = BeautifulSoup(sublist_html, 'html.parser')\n",
    "        span_list = soup_sublist.find_all('a')\n",
    "\n",
    "        # remove some unneeded url\n",
    "        temp_list = []\n",
    "        for word in span_list:\n",
    "            if(word['href'][-1] == '#' and word['href'] != '#'):\n",
    "                temp_list.append(word)\n",
    "        span_list = temp_list\n",
    "        \n",
    "        for word in span_list:\n",
    "            word_url = prefix_sub + word['href']\n",
    "            driver.get(word_url)\n",
    "            time.sleep(2)\n",
    "            word_html = driver.page_source\n",
    "            \n",
    "            soup_word = BeautifulSoup(word_html, 'html.parser')\n",
    "           \n",
    "            # lookup word \n",
    "            lookup = soup_word.find('h1', class_='hw').text\n",
    "            \n",
    "            word_pos_list= []\n",
    "            \n",
    "            # posblocks\n",
    "            posblocks = soup_word.find_all('div', class_='posblock')\n",
    "            for posblock in posblocks:\n",
    "                # pos\n",
    "                pos = posblock.find('b', class_='pos')\n",
    "                if(pos != None):\n",
    "                    pos = pos.text\n",
    "                else:\n",
    "                    print(lookup + \" has no pos.\")\n",
    "                    pos = \"\"\n",
    "                \n",
    "                word_sense_list = []\n",
    "                \n",
    "                # gwblocks\n",
    "                gwblocks = posblock.find_all('div', class_='gwblock')\n",
    "                for gwblock in gwblocks:\n",
    "                    # sense\n",
    "                    sense = gwblock.find('span', class_='def')\n",
    "                    sense_str = sense.text.strip()\n",
    "                    \n",
    "                    # level\n",
    "                    level = gwblock.find('span', class_=re.compile('freq-'))\n",
    "                    level = level.text\n",
    "                    \n",
    "                    # dict examp\n",
    "                    dict_examp = gwblock.find('blockquote', class_='examp')\n",
    "                    if(dict_examp != None):\n",
    "                        dict_examp_str = remove_char(dict_examp.text.strip())\n",
    "                    else:\n",
    "                        dict_examp_str = \"\"\n",
    "                        \n",
    "                    # learner examp\n",
    "                    lear_examp = gwblock.find('blockquote', class_='clc')\n",
    "                    if(lear_examp != None):\n",
    "                        lear_examp_str = lear_examp.text.strip()\n",
    "                        lear_examp.div.decompose() # remove \"Learner example:\"\n",
    "                        try: # some learn'examp directly comes up with examp\n",
    "                            lear_examp.div.decompose() # remove examp come from    \n",
    "                        except:\n",
    "                            pass\n",
    "                        lear_examp_str = remove_char(lear_examp.text.strip())\n",
    "                    else:\n",
    "                        lear_examp_str = \"\"\n",
    "                        \n",
    "                    word_sense = Sense(sense_str, level, dict_examp_str, lear_examp_str)\n",
    "                    word_sense_list.append(word_sense)\n",
    "                word_pos = POS(pos, word_sense_list)\n",
    "                word_pos_list.append(word_pos)\n",
    "            word_vocalbulary = Word(lookup, word_pos_list)\n",
    "            word_vocalbulary_list.append(word_vocalbulary)\n",
    "            \n",
    "    with open( 'data/' + alpha + '.json', 'w') as outfile:  \n",
    "        outfile.write(json.dumps([item.reprJSON() for item in word_vocalbulary_list], cls=ComplexEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temp test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing A\n",
      "processing B\n",
      "processing C\n",
      "processing D\n",
      "processing E\n",
      "processing F\n",
      "processing G\n",
      "processing H\n",
      "processing I\n",
      "processing J\n",
      "processing K\n",
      "processing L\n",
      "processing M\n",
      "processing N\n",
      "processing O\n",
      "processing P\n",
      "processing Q\n",
      "processing R\n",
      "processing S\n",
      "processing T\n",
      "processing U\n",
      "processing V\n",
      "processing W\n",
      "processing X\n",
      "processing Y\n",
      "processing Z\n"
     ]
    }
   ],
   "source": [
    "word_lists = []\n",
    "\n",
    "for index, alpha in enumerate(alpha_list):\n",
    "    print('processing %s' % alpha)\n",
    "    if alpha != 'S': continue #miss i\n",
    "    alpha_url = prefix_alpha + alpha\n",
    "    driver.get(alpha_url)\n",
    "    time.sleep(2)\n",
    "    alpha_html = driver.page_source\n",
    "    \n",
    "    soup_a2z = BeautifulSoup(alpha_html, 'html.parser')\n",
    "    alpha_list = soup_a2z.find_all('a')\n",
    "    \n",
    "    # remove some unneeded url\n",
    "    temp_list = []\n",
    "    for sublist in alpha_list:\n",
    "        if (re.match('/dictionary/word-list/us/a1_c2/' + alpha + '/', sublist['href'])):\n",
    "            temp_list.append(sublist)\n",
    "    alpha_list = temp_list\n",
    "    \n",
    "    word_vocalbulary_list = []\n",
    "    \n",
    "    for sublist in alpha_list:\n",
    "        # crawl every sublist\n",
    "        sublist_url = prefix_sub + sublist['href']\n",
    "        driver.get(sublist_url)\n",
    "        time.sleep(2)\n",
    "        sublist_html = driver.page_source\n",
    "\n",
    "        soup_sublist = BeautifulSoup(sublist_html, 'html.parser')\n",
    "        span_list = soup_sublist.find_all('a')\n",
    "\n",
    "        # remove some unneeded url\n",
    "        temp_list = []\n",
    "        for word in span_list:\n",
    "            if(word['href'][-1] == '#' and word['href'] != '#'):\n",
    "                # get all word's href\n",
    "                word_lists.append(prefix_sub+word['href'])\n",
    "                temp_list.append(word)\n",
    "        span_list = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, word in enumerate(word_lists):\n",
    "    if i < 580: continue\n",
    "    word_url = word\n",
    "    driver.get(word_url)\n",
    "    time.sleep(2)\n",
    "    word_html = driver.page_source\n",
    "\n",
    "    soup_word = BeautifulSoup(word_html, 'html.parser')\n",
    "\n",
    "    # lookup word \n",
    "    lookup = soup_word.find('h1', class_='hw').text\n",
    "\n",
    "    word_pos_list= []\n",
    "\n",
    "    # posblocks\n",
    "    posblocks = soup_word.find_all('div', class_='posblock')\n",
    "    for posblock in posblocks:\n",
    "        # pos\n",
    "        pos = posblock.find('b', class_='pos')\n",
    "        if(pos != None):\n",
    "            pos = pos.text\n",
    "        else:\n",
    "            print(lookup + \" has no pos.\")\n",
    "            pos = \"\"\n",
    "\n",
    "        word_sense_list = []\n",
    "\n",
    "        # gwblocks\n",
    "        gwblocks = posblock.find_all('div', class_='gwblock')\n",
    "        for gwblock in gwblocks:\n",
    "            # sense\n",
    "            sense = gwblock.find('span', class_='def')\n",
    "            sense_str = sense.text.strip()\n",
    "\n",
    "            # level\n",
    "            level = gwblock.find('span', class_=re.compile('freq-'))\n",
    "            level = level.text\n",
    "\n",
    "            # dict examp\n",
    "            dict_examp = gwblock.find('blockquote', class_='examp')\n",
    "            if(dict_examp != None):\n",
    "                dict_examp_str = remove_char(dict_examp.text.strip())\n",
    "            else:\n",
    "                dict_examp_str = \"\"\n",
    "\n",
    "            # learner examp\n",
    "            lear_examp = gwblock.find('blockquote', class_='clc')\n",
    "            if(lear_examp != None):\n",
    "                lear_examp_str = lear_examp.text.strip()\n",
    "                lear_examp.div.decompose() # remove \"Learner example:\"\n",
    "                try: # some learn'examp directly comes up with examp\n",
    "                    lear_examp.div.decompose() # remove examp come from    \n",
    "                except:\n",
    "                    pass\n",
    "                lear_examp_str = remove_char(lear_examp.text.strip())\n",
    "            else:\n",
    "                lear_examp_str = \"\"\n",
    "\n",
    "            word_sense = Sense(sense_str, level, dict_examp_str, lear_examp_str)\n",
    "            word_sense_list.append(word_sense)\n",
    "        word_pos = POS(pos, word_sense_list)\n",
    "        word_pos_list.append(word_pos)\n",
    "    word_vocalbulary = Word(lookup, word_pos_list)\n",
    "    word_vocalbulary_list.append(word_vocalbulary)\n",
    "\n",
    "with open( 'data/' + 'S' + '.json', 'w') as outfile:  \n",
    "    outfile.write(json.dumps([item.reprJSON() for item in word_vocalbulary_list], cls=ComplexEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vocalbulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
