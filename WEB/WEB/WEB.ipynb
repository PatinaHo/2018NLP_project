{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, url_for, send_file, jsonify\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils.extract import *\n",
    "from utils.voc_grading_and_detail import *\n",
    "from utils.create_pdf import *\n",
    "from utils.grammar_pattern import *\n",
    "from utils.autoFindPattern import *\n",
    "from readability import Document\n",
    "\n",
    "from utils.create_pdf.create_flashcard import *\n",
    "from utils.create_pdf.create_article import *\n",
    "from utils.create_pdf.create_wordlist import *\n",
    "from utils.create_pdf.create_grammar import *\n",
    "from utils.create_pdf.stylesheet import *\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read statistics file\n",
    "# wordCoreEXDict[word][pat] = ([colls...], (en, ch, source))\n",
    "with open('utils/data/autoFindPattern/word(V).txt', 'r') as file:\n",
    "    dictV = eval(file.read())\n",
    "with open('utils/data/autoFindPattern/phrase(V).txt', 'r') as file:\n",
    "    phraseV = eval(file.read())\n",
    "    \n",
    "# file = open('utils/data/autoFindPattern/statistics(V).txt', 'r')\n",
    "# dictV =  defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))) \n",
    "# for line in file:\n",
    "#     word, subDict = line.split('\\t')\n",
    "#     dictV[word] = eval(subDict)\n",
    "# file.close()\n",
    "\n",
    "file = open('utils/data/autoFindPattern/statistics(N).txt', 'r')\n",
    "dictN =  defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))) \n",
    "for line in file:\n",
    "    word, subDict = line.split('\\t')\n",
    "    dictN[word] = eval(subDict)\n",
    "file.close()\n",
    "\n",
    "file = open('utils/data/autoFindPattern/statistics(Adj).txt', 'r')\n",
    "dictAdj =  defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))) \n",
    "for line in file:\n",
    "    word, subDict = line.split('\\t')\n",
    "    dictAdj[word] = eval(subDict)\n",
    "file.close()\n",
    "\n",
    "\n",
    "# read statistic key\n",
    "file = open('utils/data/autoFindPattern/keys(V).txt', 'r')\n",
    "verb_set = eval(file.readline())\n",
    "file.close()\n",
    "\n",
    "file = open('utils/data/autoFindPattern/keys(N).txt', 'r')\n",
    "noun_set = {}#eval(file.readline())\n",
    "file.close()\n",
    "\n",
    "file = open('utils/data/autoFindPattern/keys(Adj).txt', 'r')\n",
    "adj_set = {}#eval(file.readline())\n",
    "file.close()\n",
    "\n",
    "\n",
    "# read translation\n",
    "TRANS = defaultdict(lambda: defaultdict(lambda: list())) # tran[pos][word] = [translation...]\n",
    "file = open('utils/data/wordTranslation/V_word_translation.txt', 'r')\n",
    "file = file.readlines()\n",
    "for line in file:\n",
    "    word, trans = line.split('\\t')\n",
    "    trans = trans.split(',')[:-1]\n",
    "    for tran in trans:\n",
    "        TRANS['V'][word].append(tran)\n",
    "        \n",
    "file = open('utils/data/wordTranslation/N_word_translation.txt', 'r')\n",
    "file = file.readlines()\n",
    "for line in file:\n",
    "    word, trans = line.split('\\t')\n",
    "    trans = trans.split(',')[:-1]\n",
    "    for tran in trans:\n",
    "        TRANS['N'][word].append(tran)\n",
    "        \n",
    "file = open('utils/data/wordTranslation/ADJ_word_translation.txt', 'r')\n",
    "file = file.readlines()\n",
    "for line in file:\n",
    "    word, trans = line.split('\\t')\n",
    "    trans = trans.split(',')[:-1]\n",
    "    for tran in trans:\n",
    "        TRANS['ADJ'][word].append(tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/Jul/2018 21:31:25] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2018 21:31:29] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2018 21:31:51] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2018 21:32:00] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2018 21:32:06] \"POST /ajax HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__ )\n",
    "import datetime\n",
    "stylesheet = stylesheet() # pdf stylesheet\n",
    "# egp = load_egp() # grammar pattern\n",
    "\n",
    "if not os.path.exists('download'):\n",
    "    os.makedirs('download')\n",
    "\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "    #return render_template('format.html', title=title, publish_date=publish_date, content=new, user_level=user_level, grade=grade)\n",
    "\n",
    "@app.route('/handle_data', methods=['POST', 'GET'])\n",
    "def handle_data():\n",
    "    url = request.form['url']\n",
    "    user_level = request.form['user_level']\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    \n",
    "    doc = Document(remove_a(response.text))\n",
    "    title = doc.short_title()\n",
    "    publish_date = getPublishDate(url)\n",
    "    content = clean_content(doc.summary())\n",
    "    \n",
    "    #grade, wordlist = voc_grading_and_detail(content, user_level)\n",
    "    #patterns = findGramPat(content)\n",
    "    # create pdf\n",
    "    #new = create_article(title, content, stylesheet, grade, 'download/'+title+'_article.pdf')\n",
    "    #print(content)\n",
    "    \n",
    "    new = create_article(title, user_level, content, stylesheet,  'download/'+title+'_article.pdf', verb_set, noun_set, adj_set)\n",
    "    \n",
    "    #create_wordlist(wordlist, patterns, 'download/'+title+'_wordlist.pdf')\n",
    "    # create_grammar(title, original, stylesheet, egp, 'download/'+title+'_grammar.pdf')\n",
    "    \n",
    "    return render_template('format.html', title=title, publish_date=publish_date, user_level=user_level, content=new) #, user_level=user_level , grade=grade\n",
    "\n",
    "@app.route('/download/<filename>', methods=['GET'])\n",
    "def return_reformatted(filename):\n",
    "    try:\n",
    "        return send_file('download/'+filename)# , as_attachment=True\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/ajax', methods = ['POST'])\n",
    "def ajax_request():\n",
    "    # 改 form request抽調\n",
    "    word = request.form['word'].lower() if request.form['pos'] != 'x' else request.form['word'].split()[0].lower()  \n",
    "    pos = [request.form['pos']] if request.form['pos'] != 'x' else [p.upper() for p in request.form['word'].split()[1:]]\n",
    "    \n",
    "    # patternTable[pos] = [(pat, colls, (en, ch, source)), ...] \n",
    "    patternTable = defaultdict(lambda: [])\n",
    "    # phraseTable[pos][phrase] = [pat, (colls, (en, ch, source)), ...] \n",
    "    phraseTable = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    # phraseOrder = [phrase...]\n",
    "    phraseOrder = []\n",
    "    # trans[pos] = [translation]\n",
    "    trans = defaultdict(lambda: defaultdict(lambda: set())) \n",
    "    \n",
    "    if word in dictV.keys():\n",
    "        # TODO須處理個數，以後可能動態\n",
    "        for pat, colls, examp in dictV[word][:5]:\n",
    "            patternTable['V'] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "        \n",
    "    if word in phraseV.keys():\n",
    "        # 前面以過濾過phrase至多3個, pat已用std過濾\n",
    "        phraseOrder = sorted(phraseV[word].keys(), key=lambda x: -int(x.rsplit('%', 1)[1]))\n",
    "        for phrase in phraseOrder:\n",
    "            for pat, colls, examp in phraseV[word][phrase]:\n",
    "                phraseTable['V'][phrase] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "    \n",
    "    if word in TRANS['V'].keys():\n",
    "        trans['V'] = TRANS['V'][word]\n",
    "                             \n",
    "    return jsonify(patternTable=patternTable, \\\n",
    "                   phraseTable=phraseTable, phraseOrder=phraseOrder, \\\n",
    "                   trans=trans)\n",
    "\n",
    "#     targetList = []\n",
    "#     targetDict = dict()\n",
    "#     if 'V' in pos: targetList.append(dictV)\n",
    "#     if 'N' in pos: targetList.append(dictN)\n",
    "#     if 'ADJ' in pos: targetList.append(dictAdj)\n",
    "#     if not targetList: targetList = [dictV, dictN, dictAdj]\n",
    "    \n",
    "#     result = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: ''))) # result[pos][pat][obj] = highlight\n",
    "#     patPair = defaultdict(lambda: []) # patPair[pos] = [pat...]\n",
    "#     objs = defaultdict(lambda: defaultdict(lambda: [])) # objs[pos][pat] = [obj...]\n",
    "#     trans = defaultdict(lambda: defaultdict(lambda: set())) # trans[pos] = [translation]\n",
    "\n",
    "#     for targetDict in targetList:\n",
    "#         if targetDict == dictV: mark = 'V'\n",
    "#         elif targetDict == dictN: mark = 'N'\n",
    "#         else: mark = 'ADJ'\n",
    "            \n",
    "#         if word in targetDict.keys():\n",
    "#             trans[mark] = TRANS[mark][word]\n",
    "#             patPair[mark] = sorted(targetDict[word].keys(), key=lambda x: -(int(x.rsplit('%', 1)[1])))[:5]\n",
    "#             subpatPair = patPair[mark]\n",
    "            \n",
    "#             sub_objs = defaultdict(lambda: []) # objs[pat] = [obj...]\n",
    "\n",
    "#             for pat in subpatPair:\n",
    "#                 objPair = sorted(targetDict[word][pat].keys(), key=lambda x: -(int(x.rsplit('%', 1)[1])))[:3]\n",
    "#                 # move '-' to list end\n",
    "#                 emptys = [pair for pair in objPair if pair.startswith('-')]\n",
    "#                 for empty in emptys:\n",
    "#                     objPair.remove(empty)\n",
    "#                     objPair += [empty]\n",
    "\n",
    "#                 sub_objs[pat] += objPair\n",
    "#                 for obj in objPair:\n",
    "#                     highlight = sorted(targetDict[word][pat][obj].items(), key=lambda x: -x[1])[0]\n",
    "#                     result[mark][pat][obj] = highlight[0]\n",
    "                    \n",
    "#             objs[mark] = sub_objs\n",
    "                      \n",
    "#     return jsonify(table=result, patterns=patPair, objs=objs, trans=trans)\n",
    "\n",
    "#static url cache buster\n",
    "@app.context_processor\n",
    "def override_url_for():\n",
    "    return dict(url_for=dated_url_for)\n",
    "\n",
    "def dated_url_for(endpoint, **values):\n",
    "    if endpoint == 'static':\n",
    "        filename = values.get('filename', None)\n",
    "        if filename:\n",
    "            file_path = os.path.join(app.root_path,\n",
    "                                     endpoint, filename)\n",
    "            values['q'] = int(os.stat(file_path).st_mtime)\n",
    "    return url_for(endpoint, **values)   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n",
    "    #app.run(host='0.0.0.0', port=int(\"9487\"), debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".flask",
   "language": "python",
   "name": ".flask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
