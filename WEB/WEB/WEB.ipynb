{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, url_for, send_file, jsonify\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from utils.extract import *\n",
    "from utils.create_pdf import *\n",
    "from readability import Document\n",
    "\n",
    "from utils.create_pdf.create_article import *\n",
    "from utils.GenerateMCQ import *        # import quiz generation\n",
    "\n",
    "import youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictWord = eval(open('utils/data/autoFindPattern/GPs.txt', 'r').read())\n",
    "phraseV = eval(open('utils/data/autoFindPattern/phrase.txt', 'r').read())\n",
    "\n",
    "# read translation\n",
    "TRANS = eval(open('utils/data/final TRANS.txt', 'r').read()) # tran[pos][word] = [translation...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5566/ (Press CTRL+C to quit)\n",
      "INFO:werkzeug: * Running on http://0.0.0.0:5566/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Jan/2019 23:46:36] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:46:36] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:46:37] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:46:37] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:47:31] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:47:31] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:47:31] \"GET /static/js/GPtable.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:47:31] \"GET /static/js/GPtable.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:47:32] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:47:32] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:47:32] \"GET /static/img/n_burned.png HTTP/1.1\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:47:32] \"GET /static/img/n_burned.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:11] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:11] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:12] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:12] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:13] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:13] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:13] \"GET /static/img/v_burned.png HTTP/1.1\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:13] \"GET /static/img/v_burned.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:14] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:14] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:16] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:16] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:20] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:20] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:20] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:20] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:20] \"GET /static/img/adj_burned.png HTTP/1.1\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:20] \"GET /static/img/adj_burned.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:30] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:30] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:31] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:31] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:49:32] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:49:32] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:51:08] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:51:08] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:51:19] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:51:19] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:51:20] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:51:20] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:51:21] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:51:21] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:51:23] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:51:23] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:51:35] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:51:35] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Jan/2019 23:51:36] \"POST /ajax HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Jan/2019 23:51:36] \"POST /ajax HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, url_for, send_file, jsonify\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from utils.extract import *\n",
    "from utils.create_pdf import *\n",
    "from readability import Document\n",
    "\n",
    "from utils.create_pdf.create_article import *\n",
    "from utils.GenerateMCQ import *        # import quiz generation\n",
    "\n",
    "import youtube_dl\n",
    "\n",
    "dictWord = eval(open('utils/data/autoFindPattern/GPs.txt', 'r').read())\n",
    "phraseV = eval(open('utils/data/autoFindPattern/phrase.txt', 'r').read())\n",
    "\n",
    "# read translation\n",
    "TRANS = eval(open('utils/data/final TRANS.txt', 'r').read()) # tran[pos][word] = [translation...]\n",
    "\n",
    "app = Flask(__name__ )\n",
    "import datetime\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "if not os.path.exists('download'):\n",
    "    os.makedirs('download')\n",
    "\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "    #return render_template('format.html', title=title, publish_date=publish_date, content=new, user_level=user_level, grade=grade)\n",
    "\n",
    "def store(*values):  # store value from handle_data() and pass to quiz() \n",
    "    store.values = values or store.values\n",
    "    return store.values    \n",
    "   \n",
    "@app.route('/handle_data', methods=['POST', 'GET'])\n",
    "def handle_data():\n",
    "    def cleancap(raw_cap):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_cap)\n",
    "        tmp = cleantext.split('\\n')\n",
    "        cap = list()\n",
    "        pre = ''\n",
    "        for line in tmp:\n",
    "            if line.replace(' ', '') and line != pre:\n",
    "                if '-->' in line: cap.append('')\n",
    "                else: pre = line\n",
    "                cap.append(line)\n",
    "        tmp = set()\n",
    "        for idx in range(len(cap)):\n",
    "            if '-->' in cap[idx] and (idx >= len(cap)-2 or '-->' in cap[idx+2]):\n",
    "                tmp.add(idx)\n",
    "                tmp.add(idx+1)\n",
    "        final = list()\n",
    "        for idx in range(len(cap)):\n",
    "            if idx not in tmp: final.append(cap[idx])\n",
    "        return '\\n'.join(final)\n",
    "    \n",
    "    user_level = request.form['user_level']\n",
    "    title = ''\n",
    "    publish_date = ''\n",
    "    text = request.form['text']\n",
    "    if (text.startswith('http://www.youtube.com')\n",
    "        or text.startswith('http://youtube.com') \n",
    "        or text.startswith('http://youtu.be') \n",
    "        or text.startswith('https://www.youtube.com') \n",
    "        or text.startswith('https://youtube.com') \n",
    "        or text.startswith('https://youtu.be')):\n",
    "        ydl_opts = {\n",
    "            'writesubtitles': True,\n",
    "            'writeautomaticsub': True,\n",
    "            'skip_download': True, # We just want to extract the info\n",
    "            'outtmpl': 'download/target' # file_path/target\n",
    "        }\n",
    "        file = ''\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([text])\n",
    "            dirPath = \"download\"\n",
    "            fileList = os.listdir(dirPath)\n",
    "            if 'target.en.vtt' in fileList:\n",
    "                file = cleancap(open('download/target.en.vtt').read())\n",
    "            else:\n",
    "                file = 'There is no english substitle in this video!'\n",
    "            for fileName in fileList:\n",
    "                if os.path.isfile(os.path.join(dirPath, fileName)): os.remove(os.path.join(dirPath, fileName))\n",
    "        v_id = text.split('=')[-1]\n",
    "        content = [v_id, file]\n",
    "        type_ = 'youtube'\n",
    "        r = requests.get(text)\n",
    "        if r.status_code < 400:\n",
    "            title = BeautifulSoup(r.text, 'html.parser').find('title').text\n",
    "            publish_date = BeautifulSoup(r.text, 'html.parser').find('meta', itemprop=\"datePublished\")['content']\n",
    "    elif text.startswith('http://') or text.startswith('https://'):\n",
    "        response = requests.get(text, headers=headers)\n",
    "        doc = Document(remove_sometag(response.text))\n",
    "        title = doc.short_title()\n",
    "        publish_date = getPublishDate(response.content.decode('UTF-8'))\n",
    "        content = doc.summary()\n",
    "        type_ = 'url'\n",
    "    else:\n",
    "        content = text\n",
    "        type_ = 'text'\n",
    "            \n",
    "    content = clean_content(content, type_)\n",
    "    wiki_link_content = add_wiki_link(content)\n",
    "    new,pure_text,vocab_dict = create_article(title, user_level, content, type_=='youtube', \\\n",
    "                         set(dictWord['V'].keys()), set(dictWord['N'].keys()), set(dictWord['ADJ'].keys()))\n",
    "    store(pure_text,vocab_dict,user_level)\n",
    "    return render_template('format.html', title=title, publish_date=publish_date, \\\n",
    "                           user_level=user_level, content=new, wiki_link_content = wiki_link_content)\n",
    "\n",
    "@app.route('/index2', methods=['POST', 'GET'])\n",
    "def quiz():\n",
    "    pure_text,vocab_dict,user_level = store() \n",
    "    if(len(pure_text) == 0):\n",
    "        con = \"\\nplease paste link or text\"\n",
    "        return render_template('format2.html', title=\"quiz\", publish_date=\"2018.8.11\", \\\n",
    "                           user_level=\"B\", content=con)    \n",
    "    tmpDict = extractVocList2(vocab_dict,user_level,10)  #extract vocabulary list \n",
    "    o = shuffle_vocab_dict(tmpDict,10)  # randomly pick up n vocabularies\n",
    "    questionDict, orderDict, pro_num, category = generateMCQ(o, 0, user_level,pure_text)\n",
    "    type_ = \"text\"\n",
    "    q = merge_two_dicts(questionDict,orderDict)\n",
    "    vocab = transformFormat(q, type_ == 'youtube', \\\n",
    "                            set(dictWord['V'].keys()), set(dictWord['N'].keys()), set(dictWord['ADJ'].keys()))\n",
    "    generateWeb(questionDict,orderDict,pro_num,category,vocab,pure_text)  # generate web file(html+js)\n",
    "    file = open(\"./templates/index2.html\", \"r\", encoding=\"utf-8\")  \n",
    "    con = file.read() # read html and js file and write into format2.html\n",
    "    return render_template('format2.html', title=\"quiz\", publish_date=\"2018.8.11\", \\\n",
    "                           user_level=\"B\", content=con)                        \n",
    "                           \n",
    "@app.route('/download/<filename>', methods=['GET'])\n",
    "def return_reformatted(filename):\n",
    "    try:\n",
    "        return send_file('download/'+filename)# , as_attachment=True\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/ajax', methods = ['POST'])\n",
    "def ajax_request():\n",
    "    word = request.form['word'].lower() if request.form['pos'] != 'x' else request.form['word'].split()[0].lower()  \n",
    "    \n",
    "    if request.form['pos'] != 'x': # click\n",
    "        poses = [request.form['pos']]\n",
    "    elif len(request.form['word'].split()) == 1: # search\n",
    "        poses = ['V', 'N', 'ADJ']\n",
    "    else:\n",
    "        poses = [p.upper() for p in request.form['word'].split()[1:]]\n",
    "    \n",
    "    finalWord = word\n",
    "    # patternTable[pos] = [(pat, colls, (en, ch, source)), ...] \n",
    "    patternTable = defaultdict(lambda: [])\n",
    "    # phraseTable[pos][phrase] = [pat, (colls, (en, ch, source)), ...] \n",
    "    phraseTable = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    # phraseOrder = [phrase...]\n",
    "    phraseOrder = []\n",
    "    # trans[type][pos] = [translation]\n",
    "    trans = defaultdict(lambda: defaultdict(lambda: list())) \n",
    "    \n",
    "    for pos in poses:\n",
    "        if pos == 'null': continue\n",
    "        if word in dictWord[pos].keys():\n",
    "            # TODO須處理個數，以後可能動態\n",
    "            for pat, colls, examp in dictWord[pos][word][:5]:\n",
    "                patternTable[pos] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "\n",
    "        if pos == 'V' and word in phraseV.keys():\n",
    "            # 前面以過濾過phrase至多3個, pat已用std過濾\n",
    "            phraseOrder = sorted(phraseV[word].keys(), key=lambda x: -int(x.rsplit('%', 1)[1]))\n",
    "            for phrase in phraseOrder:\n",
    "                for pat, colls, examp in phraseV[word][phrase]:\n",
    "                    phraseTable[pos][phrase] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                    phrase = phrase.split('%')[0]\n",
    "                    if phrase in TRANS['phrase'][pos].keys():\n",
    "                        trans['phrase'][phrase] = TRANS['phrase'][pos][phrase]\n",
    "                    else:\n",
    "                        trans['phrase'][phrase] = []\n",
    "        if finalWord in set(TRANS['pat'][pos].keys()):\n",
    "            trans['pat'][pos] = TRANS['pat'][pos][finalWord]\n",
    "        else:\n",
    "            trans['pat'][pos] = []\n",
    "    \n",
    "    if not patternTable.keys():\n",
    "        for pos in poses:\n",
    "            if pos == 'null': continue\n",
    "            if finalWord == word or not finalWord: finalWord = wordnet(word, pos, set(dictWord[pos].keys()))\n",
    "            if finalWord and finalWord != word:\n",
    "                if finalWord in dictWord[pos].keys():\n",
    "                    for pat, colls, examp in dictWord[pos][finalWord][:5]:\n",
    "                        patternTable[pos] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                        \n",
    "                if pos == 'V' and finalWord in phraseV.keys():\n",
    "                    # 前面以過濾過phrase至多3個, pat已用std過濾\n",
    "                    phraseOrder = sorted(phraseV[finalWord].keys(), key=lambda x: -int(x.rsplit('%', 1)[1]))\n",
    "                    for phrase in phraseOrder:\n",
    "                        for pat, colls, examp in phraseV[finalWord][phrase]:\n",
    "                            phraseTable[pos][phrase] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                            phrase = phrase.split('%')[0]\n",
    "                            if phrase in TRANS['phrase'][pos].keys():\n",
    "                                trans['phrase'][phrase] = TRANS['phrase'][pos][phrase]\n",
    "                            else:\n",
    "                                trans['phrase'][phrase] = []\n",
    "                if finalWord in set(TRANS['pat'][pos].keys()):\n",
    "                    trans['pat'][pos] = TRANS['pat'][pos][finalWord]\n",
    "                else:\n",
    "                    trans['pat'][pos] = []\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    return jsonify(finalWord=finalWord, \\\n",
    "                   change=(finalWord!=word), \\\n",
    "                   patternTable=patternTable, \\\n",
    "                   phraseTable=phraseTable, phraseOrder=phraseOrder, \\\n",
    "                   trans=trans)\n",
    "\n",
    "#static url cache buster\n",
    "@app.context_processor\n",
    "def override_url_for():\n",
    "    return dict(url_for=dated_url_for)\n",
    "\n",
    "def dated_url_for(endpoint, **values):\n",
    "    if endpoint == 'static':\n",
    "        filename = values.get('filename', None)\n",
    "        if filename:\n",
    "            file_path = os.path.join(app.root_path,\n",
    "                                     endpoint, filename)\n",
    "            values['q'] = int(os.stat(file_path).st_mtime)\n",
    "    return url_for(endpoint, **values)   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     app.run(debug=False)\n",
    "    app.run(host='0.0.0.0', port=int(\"5566\"), debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from nltk import sent_tokenize\n",
    "import wikipediaapi\n",
    "\n",
    "def add_link(text):\n",
    "    \n",
    "    json_request = { \"text\": text, \"spans\": []  }\n",
    "    response = requests.post(\"http://thor.nlplab.cc:5555\", json=json_request)\n",
    "    wiki = wikipediaapi.Wikipedia('en')\n",
    "    keywords_description = {}\n",
    "    for start, str_len, wiki_page in response.json():\n",
    "        if wiki_page not in keywords_description:\n",
    "            page = wiki.page(wiki_page)\n",
    "            keywords_description[wiki_page] = sent_tokenize(page.summary)[0]\n",
    "    \n",
    "    result = \"\"\n",
    "    start_idx = 0\n",
    "    wiki_url_pre = \"https://en.wikipedia.org/wiki/\"\n",
    "    for start, str_len, wiki_page in response.json():\n",
    "        result += text[start_idx:start]\n",
    "        page = (\"\"\"<a target=\"_blank\" href=\"%s\" title=\"%s\" data-toggle=\"popover\" data-trigger=\"hover\" data-placement=\"bottom\" data-content=\"%s\">%s</a>\"\"\") %(wiki_url_pre + wiki_page, wiki_page, keywords_description[wiki_page],text[start: start+str_len])\n",
    "        result+=page\n",
    "\n",
    "        start_idx = start + str_len\n",
    "    result += text[start_idx: len(text)]\n",
    "    return result\n",
    "\n",
    "def add_wiki_link(content):\n",
    "    \n",
    "    wiki_content = []\n",
    "    for tag, sentences in content:\n",
    "        \n",
    "        wiki_sentences = []\n",
    "        for idx in range(len(sentences)):\n",
    "            wiki_sentences.append(add_link(sentences[idx]))\n",
    "        wiki_content.append( [tag, ' '.join(wiki_sentences)] )\n",
    "        \n",
    "    \n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['p',\n",
       "  'Basketball star <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/LeBron_James\" title=\"LeBron_James\">LeBron James</a> has joined <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Los_Angeles_Lakers\" title=\"Los_Angeles_Lakers\">Los Angeles Lakers</a> in a four-year deal worth $154m (£116m).'],\n",
       " ['p',\n",
       "  'The 33-year-old, who has played in eight consecutive <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> finals, moves to the <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Los_Angeles_Lakers\" title=\"Los_Angeles_Lakers\">Lakers</a> from <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cleveland_Cavaliers\" title=\"Cleveland_Cavaliers\">Cleveland Cavaliers</a>.'],\n",
       " ['p',\n",
       "  'James, who became a free agent on 1 July, is widely considered the best basketball player in the world.'],\n",
       " ['p',\n",
       "  '\"Thank you Northeast Ohio for an incredible four seasons,\" said James on his Instagram account. \"This will always be home.\"'],\n",
       " ['p',\n",
       "  'The three-time <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> champion was selected by the Cavaliers in 2003 as the first pick in the player draft and established himself as one of the league\\'s best players. He was named the <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a>\\'s most valuable player in 2009 and 2010 and controversially moved to <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Miami\" title=\"Miami\">Miami</a> in 2010.'],\n",
       " ['p',\n",
       "  'James won his first <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> title in 2012 and added a second championship the following year.'],\n",
       " ['p',\n",
       "  'He then opted out of the final two years of his <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Miami\" title=\"Miami\">Miami</a> contract and returned to <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cleveland\" title=\"Cleveland\">Cleveland</a>.'],\n",
       " ['p',\n",
       "  'James helped Cleveland to their first <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> title in 2016, as they overturned a 3-1 deficit in the <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> finals to beat <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Golden_State_Warriors\" title=\"Golden_State_Warriors\">Golden State</a>.'],\n",
       " ['p',\n",
       "  \"Cleveland's success also ended the city's 52-year wait for a major sporting title.\"]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_wiki_link(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Barack Obama's wife Michelle quickly responds on Twitter.\"\"\"\n",
    "json_request = { \"text\": text, \"spans\": []  }\n",
    "response = requests.post(\"http://thor.nlplab.cc:5555\", json=json_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 12, 'Barack_Obama'], [20, 8, 'Michelle_Obama'], [49, 7, 'Twitter']]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cleveland',\n",
       " 'Cleveland_Cavaliers',\n",
       " 'Cleveland_Indians',\n",
       " 'Golden_State_Warriors',\n",
       " 'LeBron_James',\n",
       " 'Los_Angeles_Lakers',\n",
       " 'Miami',\n",
       " 'National_Basketball_Association'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_set = set()\n",
    "for start, str_len, wiki_page in response.json():\n",
    "    entity_set.add(wiki_page)\n",
    "entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cleveland Cavaliers, often referred to as the Cavs, are an American professional basketball team based in Cleveland, Ohio.\n",
      "The Los Angeles Lakers are an American professional basketball team based in Los Angeles.\n",
      "The Golden State Warriors are an American professional basketball team based in Oakland, California.\n",
      "Cleveland ( KLEEV-lənd) is a major city in the U.S. state of Ohio, and the county seat of Cuyahoga County.\n",
      "Miami, officially the City of Miami, is the cultural, economic and financial center of South Florida.\n",
      "The Cleveland Indians are an American professional baseball team based in Cleveland, Ohio.\n",
      "The National Basketball Association (NBA) is a men's professional basketball league in North America; composed of 30 teams (29 in the United States and 1 in Canada).\n",
      "LeBron Raymone James Sr. (; born December 30, 1984) is an American professional basketball player for the Los Angeles Lakers of the National Basketball Association (NBA).\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import wikipediaapi\n",
    "wiki = wikipediaapi.Wikipedia('en')\n",
    "for entity in entity_set:\n",
    "    page = wiki.page(entity)\n",
    "    print(sent_tokenize(page.summary)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_attention = False\n",
    "with open(\"/Users/patinaho/Documents/NLPLAB/data/e.abst.txt\") as f:\n",
    "    for count, line in enumerate(f):\n",
    "        title = line.strip().strip(\"</title>\")\n",
    "        if(title in entity_set):\n",
    "            context_attention = True\n",
    "            continue\n",
    "        if(context_attention == True):\n",
    "            description = line\n",
    "#             print(description)\n",
    "            context_attention = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linggle-booster-env",
   "language": "python",
   "name": "linggle-booster-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
