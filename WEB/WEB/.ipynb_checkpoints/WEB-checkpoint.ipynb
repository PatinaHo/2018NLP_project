{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, url_for, send_file, jsonify\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from utils.extract import *\n",
    "from utils.create_pdf import *\n",
    "from readability import Document\n",
    "\n",
    "from utils.create_pdf.create_article import *\n",
    "from utils.GenerateMCQ import *        # import quiz generation\n",
    "\n",
    "import youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictWord = eval(open('utils/data/autoFindPattern/GPs.txt', 'r').read())\n",
    "phraseV = eval(open('utils/data/autoFindPattern/phrase.txt', 'r').read())\n",
    "\n",
    "# read translation\n",
    "TRANS = eval(open('utils/data/final TRANS.txt', 'r').read()) # tran[pos][word] = [translation...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5566/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [24/Jan/2019 15:46:21] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:46:23] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:46:23] \"GET /static/img/favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:46:33] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:46:33] \"GET /static/js/GPtable.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:46:35] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:46:35] \"GET /static/img/n_burned.png HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, url_for, send_file, jsonify\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from utils.extract import *\n",
    "from utils.create_pdf import *\n",
    "from readability import Document\n",
    "\n",
    "from utils.create_pdf.create_article import *\n",
    "from utils.GenerateMCQ import *        # import quiz generation\n",
    "\n",
    "import youtube_dl\n",
    "\n",
    "dictWord = eval(open('utils/data/autoFindPattern/GPs.txt', 'r').read())\n",
    "phraseV = eval(open('utils/data/autoFindPattern/phrase.txt', 'r').read())\n",
    "\n",
    "# read translation\n",
    "TRANS = eval(open('utils/data/final TRANS.txt', 'r').read()) # tran[pos][word] = [translation...]\n",
    "\n",
    "app = Flask(__name__ )\n",
    "import datetime\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "if not os.path.exists('download'):\n",
    "    os.makedirs('download')\n",
    "\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "    #return render_template('format.html', title=title, publish_date=publish_date, content=new, user_level=user_level, grade=grade)\n",
    "\n",
    "def store(*values):  # store value from handle_data() and pass to quiz() \n",
    "    store.values = values or store.values\n",
    "    return store.values    \n",
    "   \n",
    "@app.route('/handle_data', methods=['POST', 'GET'])\n",
    "def handle_data():\n",
    "    def cleancap(raw_cap):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_cap)\n",
    "        tmp = cleantext.split('\\n')\n",
    "        cap = list()\n",
    "        pre = ''\n",
    "        for line in tmp:\n",
    "            if line.replace(' ', '') and line != pre:\n",
    "                if '-->' in line: cap.append('')\n",
    "                else: pre = line\n",
    "                cap.append(line)\n",
    "        tmp = set()\n",
    "        for idx in range(len(cap)):\n",
    "            if '-->' in cap[idx] and (idx >= len(cap)-2 or '-->' in cap[idx+2]):\n",
    "                tmp.add(idx)\n",
    "                tmp.add(idx+1)\n",
    "        final = list()\n",
    "        for idx in range(len(cap)):\n",
    "            if idx not in tmp: final.append(cap[idx])\n",
    "        return '\\n'.join(final)\n",
    "    \n",
    "    user_level = request.form['user_level']\n",
    "    title = ''\n",
    "    publish_date = ''\n",
    "    text = request.form['text']\n",
    "    if (text.startswith('http://www.youtube.com')\n",
    "        or text.startswith('http://youtube.com') \n",
    "        or text.startswith('http://youtu.be') \n",
    "        or text.startswith('https://www.youtube.com') \n",
    "        or text.startswith('https://youtube.com') \n",
    "        or text.startswith('https://youtu.be')):\n",
    "        ydl_opts = {\n",
    "            'writesubtitles': True,\n",
    "            'writeautomaticsub': True,\n",
    "            'skip_download': True, # We just want to extract the info\n",
    "            'outtmpl': 'download/target' # file_path/target\n",
    "        }\n",
    "        file = ''\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([text])\n",
    "            dirPath = \"download\"\n",
    "            fileList = os.listdir(dirPath)\n",
    "            if 'target.en.vtt' in fileList:\n",
    "                file = cleancap(open('download/target.en.vtt').read())\n",
    "            else:\n",
    "                file = 'There is no english substitle in this video!'\n",
    "            for fileName in fileList:\n",
    "                if os.path.isfile(os.path.join(dirPath, fileName)): os.remove(os.path.join(dirPath, fileName))\n",
    "        v_id = text.split('=')[-1]\n",
    "        content = [v_id, file]\n",
    "        type_ = 'youtube'\n",
    "        r = requests.get(text)\n",
    "        if r.status_code < 400:\n",
    "            title = BeautifulSoup(r.text, 'html.parser').find('title').text\n",
    "            publish_date = BeautifulSoup(r.text, 'html.parser').find('meta', itemprop=\"datePublished\")['content']\n",
    "    elif text.startswith('http://') or text.startswith('https://'):\n",
    "        response = requests.get(text, headers=headers)\n",
    "        doc = Document(remove_sometag(response.text))\n",
    "        title = doc.short_title()\n",
    "        publish_date = getPublishDate(response.content.decode('UTF-8'))\n",
    "        content = doc.summary()\n",
    "        type_ = 'url'\n",
    "    else:\n",
    "        content = text\n",
    "        type_ = 'text'\n",
    "            \n",
    "    content = clean_content(content, type_)\n",
    "    wiki_link_content = add_wiki_link(content)\n",
    "    new,pure_text,vocab_dict = create_article(title, user_level, content, type_=='youtube', \\\n",
    "                         set(dictWord['V'].keys()), set(dictWord['N'].keys()), set(dictWord['ADJ'].keys()))\n",
    "    store(pure_text,vocab_dict,user_level)\n",
    "    return render_template('format.html', title=title, publish_date=publish_date, \\\n",
    "                           user_level=user_level, content=new, wiki_link_content = wiki_link_content)\n",
    "\n",
    "@app.route('/index2', methods=['POST', 'GET'])\n",
    "def quiz():\n",
    "    pure_text,vocab_dict,user_level = store() \n",
    "    if(len(pure_text) == 0):\n",
    "        con = \"\\nplease paste link or text\"\n",
    "        return render_template('format2.html', title=\"quiz\", publish_date=\"2018.8.11\", \\\n",
    "                           user_level=\"B\", content=con)    \n",
    "    tmpDict = extractVocList2(vocab_dict,user_level,10)  #extract vocabulary list \n",
    "    o = shuffle_vocab_dict(tmpDict,10)  # randomly pick up n vocabularies\n",
    "    questionDict, orderDict, pro_num, category = generateMCQ(o, 0, user_level,pure_text)\n",
    "    type_ = \"text\"\n",
    "    q = merge_two_dicts(questionDict,orderDict)\n",
    "    vocab = transformFormat(q, type_ == 'youtube', \\\n",
    "                            set(dictWord['V'].keys()), set(dictWord['N'].keys()), set(dictWord['ADJ'].keys()))\n",
    "    generateWeb(questionDict,orderDict,pro_num,category,vocab,pure_text)  # generate web file(html+js)\n",
    "    file = open(\"./templates/index2.html\", \"r\", encoding=\"utf-8\")  \n",
    "    con = file.read() # read html and js file and write into format2.html\n",
    "    return render_template('format2.html', title=\"quiz\", publish_date=\"2018.8.11\", \\\n",
    "                           user_level=\"B\", content=con)                        \n",
    "                           \n",
    "@app.route('/download/<filename>', methods=['GET'])\n",
    "def return_reformatted(filename):\n",
    "    try:\n",
    "        return send_file('download/'+filename)# , as_attachment=True\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/ajax', methods = ['POST'])\n",
    "def ajax_request():\n",
    "    word = request.form['word'].lower() if request.form['pos'] != 'x' else request.form['word'].split()[0].lower()  \n",
    "    \n",
    "    if request.form['pos'] != 'x': # click\n",
    "        poses = [request.form['pos']]\n",
    "    elif len(request.form['word'].split()) == 1: # search\n",
    "        poses = ['V', 'N', 'ADJ']\n",
    "    else:\n",
    "        poses = [p.upper() for p in request.form['word'].split()[1:]]\n",
    "    \n",
    "    finalWord = word\n",
    "    # patternTable[pos] = [(pat, colls, (en, ch, source)), ...] \n",
    "    patternTable = defaultdict(lambda: [])\n",
    "    # phraseTable[pos][phrase] = [pat, (colls, (en, ch, source)), ...] \n",
    "    phraseTable = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    # phraseOrder = [phrase...]\n",
    "    phraseOrder = []\n",
    "    # trans[type][pos] = [translation]\n",
    "    trans = defaultdict(lambda: defaultdict(lambda: list())) \n",
    "    \n",
    "    for pos in poses:\n",
    "        if pos == 'null': continue\n",
    "        if word in dictWord[pos].keys():\n",
    "            # TODO須處理個數，以後可能動態\n",
    "            for pat, colls, examp in dictWord[pos][word][:5]:\n",
    "                patternTable[pos] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "\n",
    "        if pos == 'V' and word in phraseV.keys():\n",
    "            # 前面以過濾過phrase至多3個, pat已用std過濾\n",
    "            phraseOrder = sorted(phraseV[word].keys(), key=lambda x: -int(x.rsplit('%', 1)[1]))\n",
    "            for phrase in phraseOrder:\n",
    "                for pat, colls, examp in phraseV[word][phrase]:\n",
    "                    phraseTable[pos][phrase] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                    phrase = phrase.split('%')[0]\n",
    "                    if phrase in TRANS['phrase'][pos].keys():\n",
    "                        trans['phrase'][phrase] = TRANS['phrase'][pos][phrase]\n",
    "                    else:\n",
    "                        trans['phrase'][phrase] = []\n",
    "        if finalWord in set(TRANS['pat'][pos].keys()):\n",
    "            trans['pat'][pos] = TRANS['pat'][pos][finalWord]\n",
    "        else:\n",
    "            trans['pat'][pos] = []\n",
    "    \n",
    "    if not patternTable.keys():\n",
    "        for pos in poses:\n",
    "            if pos == 'null': continue\n",
    "            if finalWord == word or not finalWord: finalWord = wordnet(word, pos, set(dictWord[pos].keys()))\n",
    "            if finalWord and finalWord != word:\n",
    "                if finalWord in dictWord[pos].keys():\n",
    "                    for pat, colls, examp in dictWord[pos][finalWord][:5]:\n",
    "                        patternTable[pos] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                        \n",
    "                if pos == 'V' and finalWord in phraseV.keys():\n",
    "                    # 前面以過濾過phrase至多3個, pat已用std過濾\n",
    "                    phraseOrder = sorted(phraseV[finalWord].keys(), key=lambda x: -int(x.rsplit('%', 1)[1]))\n",
    "                    for phrase in phraseOrder:\n",
    "                        for pat, colls, examp in phraseV[finalWord][phrase]:\n",
    "                            phraseTable[pos][phrase] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                            phrase = phrase.split('%')[0]\n",
    "                            if phrase in TRANS['phrase'][pos].keys():\n",
    "                                trans['phrase'][phrase] = TRANS['phrase'][pos][phrase]\n",
    "                            else:\n",
    "                                trans['phrase'][phrase] = []\n",
    "                if finalWord in set(TRANS['pat'][pos].keys()):\n",
    "                    trans['pat'][pos] = TRANS['pat'][pos][finalWord]\n",
    "                else:\n",
    "                    trans['pat'][pos] = []\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    return jsonify(finalWord=finalWord, \\\n",
    "                   change=(finalWord!=word), \\\n",
    "                   patternTable=patternTable, \\\n",
    "                   phraseTable=phraseTable, phraseOrder=phraseOrder, \\\n",
    "                   trans=trans)\n",
    "\n",
    "#static url cache buster\n",
    "@app.context_processor\n",
    "def override_url_for():\n",
    "    return dict(url_for=dated_url_for)\n",
    "\n",
    "def dated_url_for(endpoint, **values):\n",
    "    if endpoint == 'static':\n",
    "        filename = values.get('filename', None)\n",
    "        if filename:\n",
    "            file_path = os.path.join(app.root_path,\n",
    "                                     endpoint, filename)\n",
    "            values['q'] = int(os.stat(file_path).st_mtime)\n",
    "    return url_for(endpoint, **values)   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     app.run(debug=False)\n",
    "    app.run(host='0.0.0.0', port=int(\"5566\"), debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [24/Jan/2019 15:28:14] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:28:15] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:28:26] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:28:26] \"GET /static/js/GPtable.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:28:28] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:28:28] \"GET /static/img/n_burned.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:30:21] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:30:21] \"GET /static/img/v_burned.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:30:24] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:07] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:14] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:14] \"GET /static/js/GPtable.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:16] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:16] \"GET /static/img/n_burned.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:17] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:32] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:33] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:38] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:38] \"GET /static/js/GPtable.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:41] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:34:41] \"GET /static/img/n_burned.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:35:23] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:37:06] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:37:12] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:37:12] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:38:43] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:38:45] \"GET /static/img/welcome-bg.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:38:51] \"POST /handle_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:38:51] \"GET /static/js/GPtable.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:38:53] \"POST /ajax HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2019 15:38:53] \"GET /static/img/n_burned.png HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__ )\n",
    "import datetime\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "if not os.path.exists('download'):\n",
    "    os.makedirs('download')\n",
    "\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "    #return render_template('format.html', title=title, publish_date=publish_date, content=new, user_level=user_level, grade=grade)\n",
    "\n",
    "def store(*values):  # store value from handle_data() and pass to quiz() \n",
    "    store.values = values or store.values\n",
    "    return store.values    \n",
    "   \n",
    "@app.route('/handle_data', methods=['POST', 'GET'])\n",
    "def handle_data():\n",
    "    def cleancap(raw_cap):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_cap)\n",
    "        tmp = cleantext.split('\\n')\n",
    "        cap = list()\n",
    "        pre = ''\n",
    "        for line in tmp:\n",
    "            if line.replace(' ', '') and line != pre:\n",
    "                if '-->' in line: cap.append('')\n",
    "                else: pre = line\n",
    "                cap.append(line)\n",
    "        tmp = set()\n",
    "        for idx in range(len(cap)):\n",
    "            if '-->' in cap[idx] and (idx >= len(cap)-2 or '-->' in cap[idx+2]):\n",
    "                tmp.add(idx)\n",
    "                tmp.add(idx+1)\n",
    "        final = list()\n",
    "        for idx in range(len(cap)):\n",
    "            if idx not in tmp: final.append(cap[idx])\n",
    "        return '\\n'.join(final)\n",
    "    \n",
    "    user_level = request.form['user_level']\n",
    "    title = ''\n",
    "    publish_date = ''\n",
    "    text = request.form['text']\n",
    "    if (text.startswith('http://www.youtube.com')\n",
    "        or text.startswith('http://youtube.com') \n",
    "        or text.startswith('http://youtu.be') \n",
    "        or text.startswith('https://www.youtube.com') \n",
    "        or text.startswith('https://youtube.com') \n",
    "        or text.startswith('https://youtu.be')):\n",
    "        ydl_opts = {\n",
    "            'writesubtitles': True,\n",
    "            'writeautomaticsub': True,\n",
    "            'skip_download': True, # We just want to extract the info\n",
    "            'outtmpl': 'download/target' # file_path/target\n",
    "        }\n",
    "        file = ''\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([text])\n",
    "            dirPath = \"download\"\n",
    "            fileList = os.listdir(dirPath)\n",
    "            if 'target.en.vtt' in fileList:\n",
    "                file = cleancap(open('download/target.en.vtt').read())\n",
    "            else:\n",
    "                file = 'There is no english substitle in this video!'\n",
    "            for fileName in fileList:\n",
    "                if os.path.isfile(os.path.join(dirPath, fileName)): os.remove(os.path.join(dirPath, fileName))\n",
    "        v_id = text.split('=')[-1]\n",
    "        content = [v_id, file]\n",
    "        type_ = 'youtube'\n",
    "        r = requests.get(text)\n",
    "        if r.status_code < 400:\n",
    "            title = BeautifulSoup(r.text, 'html.parser').find('title').text\n",
    "            publish_date = BeautifulSoup(r.text, 'html.parser').find('meta', itemprop=\"datePublished\")['content']\n",
    "    elif text.startswith('http://') or text.startswith('https://'):\n",
    "        response = requests.get(text, headers=headers)\n",
    "        doc = Document(remove_sometag(response.text))\n",
    "        title = doc.short_title()\n",
    "        publish_date = getPublishDate(response.content.decode('UTF-8'))\n",
    "        content = doc.summary()\n",
    "        type_ = 'url'\n",
    "    else:\n",
    "        content = text\n",
    "        type_ = 'text'\n",
    "            \n",
    "    content = clean_content(content, type_)\n",
    "    wiki_link_content = add_wiki_link(content)\n",
    "    new,pure_text,vocab_dict = create_article(title, user_level, wiki_link_content, type_=='youtube', \\\n",
    "                         set(dictWord['V'].keys()), set(dictWord['N'].keys()), set(dictWord['ADJ'].keys()))\n",
    "    store(pure_text,vocab_dict,user_level)\n",
    "    return render_template('format.html', title=title, publish_date=publish_date, \\\n",
    "                           user_level=user_level, content=new) \n",
    "\n",
    "@app.route('/index2', methods=['POST', 'GET'])\n",
    "def quiz():\n",
    "    pure_text,vocab_dict,user_level = store()  \n",
    "    tmpDict = extractVocList2(vocab_dict,user_level,10)  #extract vocabulary list \n",
    "    o = shuffle_vocab_dict(tmpDict,10)  # randomly pick up n vocabularies\n",
    "    questionDict, orderDict, pro_num, category = generateMCQ(o, 0, user_level,pure_text)\n",
    "    type_ = \"text\"\n",
    "    q = merge_two_dicts(questionDict,orderDict)\n",
    "    vocab = transformFormat(q, type_ == 'youtube', \\\n",
    "                            set(dictWord['V'].keys()), set(dictWord['N'].keys()), set(dictWord['ADJ'].keys()))\n",
    "    generateWeb(questionDict,orderDict,pro_num,category,vocab,pure_text)  # generate web file(html+js)\n",
    "    file = open(\"./templates/index2.html\", \"r\", encoding=\"utf-8\")  \n",
    "    con = file.read() # read html and js file and write into format2.html\n",
    "    return render_template('format2.html', title=\"quiz\", publish_date=\"2018.8.11\", \\\n",
    "                           user_level=\"B\", content=con)                        \n",
    "                           \n",
    "@app.route('/download/<filename>', methods=['GET'])\n",
    "def return_reformatted(filename):\n",
    "    try:\n",
    "        return send_file('download/'+filename)# , as_attachment=True\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/ajax', methods = ['POST'])\n",
    "def ajax_request():\n",
    "    word = request.form['word'].lower() if request.form['pos'] != 'x' else request.form['word'].split()[0].lower()  \n",
    "    \n",
    "    if request.form['pos'] != 'x': # click\n",
    "        poses = [request.form['pos']]\n",
    "    elif len(request.form['word'].split()) == 1: # search\n",
    "        poses = ['V', 'N', 'ADJ']\n",
    "    else:\n",
    "        poses = [p.upper() for p in request.form['word'].split()[1:]]\n",
    "    \n",
    "    finalWord = word\n",
    "    # patternTable[pos] = [(pat, colls, (en, ch, source)), ...] \n",
    "    patternTable = defaultdict(lambda: [])\n",
    "    # phraseTable[pos][phrase] = [pat, (colls, (en, ch, source)), ...] \n",
    "    phraseTable = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    # phraseOrder = [phrase...]\n",
    "    phraseOrder = []\n",
    "    # trans[type][pos] = [translation]\n",
    "    trans = defaultdict(lambda: defaultdict(lambda: list())) \n",
    "    \n",
    "    for pos in poses:\n",
    "        if pos == 'null': continue\n",
    "        if word in dictWord[pos].keys():\n",
    "            # TODO須處理個數，以後可能動態\n",
    "            for pat, colls, examp in dictWord[pos][word][:5]:\n",
    "                patternTable[pos] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "\n",
    "        if pos == 'V' and word in phraseV.keys():\n",
    "            # 前面以過濾過phrase至多3個, pat已用std過濾\n",
    "            phraseOrder = sorted(phraseV[word].keys(), key=lambda x: -int(x.rsplit('%', 1)[1]))\n",
    "            for phrase in phraseOrder:\n",
    "                for pat, colls, examp in phraseV[word][phrase]:\n",
    "                    phraseTable[pos][phrase] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                    phrase = phrase.split('%')[0]\n",
    "                    if phrase in TRANS['phrase'][pos].keys():\n",
    "                        trans['phrase'][phrase] = TRANS['phrase'][pos][phrase]\n",
    "                    else:\n",
    "                        trans['phrase'][phrase] = []\n",
    "        if finalWord in set(TRANS['pat'][pos].keys()):\n",
    "            trans['pat'][pos] = TRANS['pat'][pos][finalWord]\n",
    "        else:\n",
    "            trans['pat'][pos] = []\n",
    "    \n",
    "    if not patternTable.keys():\n",
    "        for pos in poses:\n",
    "            if pos == 'null': continue\n",
    "            if finalWord == word or not finalWord: finalWord = wordnet(word, pos, set(dictWord[pos].keys()))\n",
    "            if finalWord and finalWord != word:\n",
    "                if finalWord in dictWord[pos].keys():\n",
    "                    for pat, colls, examp in dictWord[pos][finalWord][:5]:\n",
    "                        patternTable[pos] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                        \n",
    "                if pos == 'V' and finalWord in phraseV.keys():\n",
    "                    # 前面以過濾過phrase至多3個, pat已用std過濾\n",
    "                    phraseOrder = sorted(phraseV[finalWord].keys(), key=lambda x: -int(x.rsplit('%', 1)[1]))\n",
    "                    for phrase in phraseOrder:\n",
    "                        for pat, colls, examp in phraseV[finalWord][phrase]:\n",
    "                            phraseTable[pos][phrase] += [(pat, ', '.join(colls[:3]), examp)]\n",
    "                            phrase = phrase.split('%')[0]\n",
    "                            if phrase in TRANS['phrase'][pos].keys():\n",
    "                                trans['phrase'][phrase] = TRANS['phrase'][pos][phrase]\n",
    "                            else:\n",
    "                                trans['phrase'][phrase] = []\n",
    "                if finalWord in set(TRANS['pat'][pos].keys()):\n",
    "                    trans['pat'][pos] = TRANS['pat'][pos][finalWord]\n",
    "                else:\n",
    "                    trans['pat'][pos] = []\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    return jsonify(finalWord=finalWord, \\\n",
    "                   change=(finalWord!=word), \\\n",
    "                   patternTable=patternTable, \\\n",
    "                   phraseTable=phraseTable, phraseOrder=phraseOrder, \\\n",
    "                   trans=trans)\n",
    "\n",
    "#static url cache buster\n",
    "@app.context_processor\n",
    "def override_url_for():\n",
    "    return dict(url_for=dated_url_for)\n",
    "\n",
    "def dated_url_for(endpoint, **values):\n",
    "    if endpoint == 'static':\n",
    "        filename = values.get('filename', None)\n",
    "        if filename:\n",
    "            file_path = os.path.join(app.root_path,\n",
    "                                     endpoint, filename)\n",
    "            values['q'] = int(os.stat(file_path).st_mtime)\n",
    "    return url_for(endpoint, **values)   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n",
    "#     app.run(host='0.0.0.0', port=int(\"5487\"), debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def add_link(text):\n",
    "    \n",
    "    json_request = { \"text\": text, \"spans\": []  }\n",
    "    response = requests.post(\"http://thor.nlplab.cc:5555\", json=json_request)\n",
    "    \n",
    "    result = \"\"\n",
    "    start_idx = 0\n",
    "    wiki_url_pre = \"https://en.wikipedia.org/wiki/\"\n",
    "    for start, str_len, wiki_page in response.json():\n",
    "        result += text[start_idx:start]\n",
    "        page = (\"\"\"<a href=\"%s\" title=\"%s\">%s</a>\"\"\") %(wiki_url_pre + wiki_page, wiki_page, text[start: start+str_len])\n",
    "        result+=page\n",
    "\n",
    "        start_idx = start + str_len\n",
    "    result += text[start_idx: len(text)]\n",
    "    return result\n",
    "\n",
    "def add_wiki_link(content):\n",
    "    \n",
    "    wiki_content = []\n",
    "    for tag, sentences in content:\n",
    "        \n",
    "        wiki_sentences = []\n",
    "        for idx in range(len(sentences)):\n",
    "            wiki_sentences.append(add_link(sentences[idx]))\n",
    "        wiki_content.append( [tag, [' '.join(wiki_sentences)]] )\n",
    "        \n",
    "    \n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['p',\n",
       "  ['Basketball star <a href=\"https://en.wikipedia.org/wiki/LeBron_James\" title=\"LeBron_James\">LeBron James</a> has joined <a href=\"https://en.wikipedia.org/wiki/Los_Angeles_Lakers\" title=\"Los_Angeles_Lakers\">Los Angeles Lakers</a> in a four-year deal worth $154m (£116m).']],\n",
       " ['p',\n",
       "  ['The 33-year-old, who has played in eight consecutive <a href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> finals, moves to the <a href=\"https://en.wikipedia.org/wiki/Los_Angeles_Lakers\" title=\"Los_Angeles_Lakers\">Lakers</a> from <a href=\"https://en.wikipedia.org/wiki/Cleveland_Cavaliers\" title=\"Cleveland_Cavaliers\">Cleveland Cavaliers</a>.']],\n",
       " ['p',\n",
       "  ['James, who became a free agent on 1 July, is widely considered the best basketball player in the world.']],\n",
       " ['p',\n",
       "  ['\"Thank you Northeast Ohio for an incredible four seasons,\" said James on his Instagram account. \"This will always be home.\"']],\n",
       " ['p',\n",
       "  ['The three-time <a href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> champion was selected by the Cavaliers in 2003 as the first pick in the player draft and established himself as one of the league\\'s best players. He was named the <a href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a>\\'s most valuable player in 2009 and 2010 and controversially moved to <a href=\"https://en.wikipedia.org/wiki/Miami\" title=\"Miami\">Miami</a> in 2010.']],\n",
       " ['p',\n",
       "  ['James won his first <a href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> title in 2012 and added a second championship the following year.']],\n",
       " ['p',\n",
       "  ['He then opted out of the final two years of his <a href=\"https://en.wikipedia.org/wiki/Miami\" title=\"Miami\">Miami</a> contract and returned to <a href=\"https://en.wikipedia.org/wiki/Cleveland\" title=\"Cleveland\">Cleveland</a>.']],\n",
       " ['p',\n",
       "  ['James helped Cleveland to their first <a href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> title in 2016, as they overturned a 3-1 deficit in the <a href=\"https://en.wikipedia.org/wiki/National_Basketball_Association\" title=\"National_Basketball_Association\">NBA</a> finals to beat <a href=\"https://en.wikipedia.org/wiki/Golden_State_Warriors\" title=\"Golden_State_Warriors\">Golden State</a>.']],\n",
       " ['p',\n",
       "  [\"Cleveland's success also ended the city's 52-year wait for a major sporting title.\"]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_wiki_link(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linggle-booster-env",
   "language": "python",
   "name": "linggle-booster-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
